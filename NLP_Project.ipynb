{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StefaniaGutu/OffensiveLanguageClassification/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import spacy\n",
        "import pickle\n",
        "from os.path import exists"
      ],
      "metadata": {
        "id": "cacUqx0ZiRkd"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EilxfP8SF4iS",
        "outputId": "3f5e40a3-f218-4843-f0eb-faa310aa2f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0               0      3            0                   0        3      2   \n",
            "1               1      3            0                   3        0      1   \n",
            "2               2      3            0                   3        0      1   \n",
            "3               3      3            0                   2        1      1   \n",
            "4               4      6            0                   6        0      1   \n",
            "...           ...    ...          ...                 ...      ...    ...   \n",
            "24778       25291      3            0                   2        1      1   \n",
            "24779       25292      3            0                   1        2      2   \n",
            "24780       25294      3            0                   3        0      1   \n",
            "24781       25295      6            0                   6        0      1   \n",
            "24782       25296      3            0                   0        3      2   \n",
            "\n",
            "                                                   tweet  \n",
            "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
            "...                                                  ...  \n",
            "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
            "24779  you've gone and broke the wrong heart baby, an...  \n",
            "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
            "24781              youu got wild bitches tellin you lies  \n",
            "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
            "\n",
            "[24783 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('sample_data/labeled_data.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_labels = df[\"class\"]\n",
        "dataset_tweets = df[\"tweet\"]"
      ],
      "metadata": {
        "id": "XIYuJlaDcfpS"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocesare"
      ],
      "metadata": {
        "id": "-ZyhmHOUhFIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(dataset):\n",
        "    # remove @user\n",
        "    tweets_no_user = [re.sub(r'@[\\w]*', '', tweet) for tweet in dataset_tweets]\n",
        "\n",
        "    # remove URL\n",
        "    tweets_no_links = [re.sub(r'http\\S+', '', tweet) for tweet in tweets_no_user]\n",
        "\n",
        "    # remove hashtags\n",
        "    tweets_no_hashtags = [re.sub(r'#[a-zA-Z0-9_]+','', tweet) for tweet in tweets_no_links]\n",
        "\n",
        "    # remove &_;\n",
        "    tweets_no_apersand = [re.sub(r'&[a-zA-Z0-9_]+','', tweet) for tweet in tweets_no_hashtags]\n",
        "\n",
        "    # remove Punctuations and Special Characters\n",
        "    tweets_no_punct = [tweet.translate(str.maketrans('', '', string.punctuation)) for tweet in tweets_no_apersand]\n",
        "\n",
        "    # remove Numbers\n",
        "    tweets_no_digits = [re.sub(r'\\d+', '', tweet) for tweet in tweets_no_punct]\n",
        "\n",
        "    # Convert to lower case\n",
        "    tweets_lower = [tweet.lower() for tweet in tweets_no_digits]\n",
        "\n",
        "    # Convert more than 2 letter repetitions to 2 letter\n",
        "    tweets_no_repetitions = [re.sub(r'(.)\\1+', r'\\1\\1', tweet) for tweet in tweets_lower]\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    tweets_no_spaces = [re.sub(r'\\s+', ' ', tweet) for tweet in tweets_no_repetitions]\n",
        "\n",
        "    # Lemma\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    all_words_lemma = [[word.lemma_ for word in nlp(sent)] for sent in tweets_no_spaces]\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words_spacy = nlp.Defaults.stop_words\n",
        "    all_words_without_stops = [[word for word in sent if word not in stop_words_spacy] for sent in all_words_lemma]\n",
        "\n",
        "    # remove Short Words\n",
        "    words_no_shortWords = [[word for word in tweet if len(word)>2] for tweet in all_words_without_stops]\n",
        "    tweets_no_shortWords = [word for word in words_no_shortWords]\n",
        "\n",
        "    return tweets_no_shortWords"
      ],
      "metadata": {
        "id": "5udO_XE6hG_B"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_dataset = preprocess(dataset_tweets)\n",
        "\n",
        "print(preprocessed_dataset[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pg5mbsBzviN",
        "outputId": "73286adf-38b4-4f0c-89e5-a4a6430ae1c2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['woman', 'complain', 'clean', 'house', 'man', 'trash'], ['boy', 'dat', 'coldtyga', 'dwn', 'bad', 'cuffin', 'dat', 'hoe', 'place'], ['dawg', 'fuck', 'bitch', 'start', 'cry', 'confuse', 'shit'], ['look', 'like', 'tranny'], ['shit', 'hear', 'true', 'faker', 'bitch', 'tell']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_dataset[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsP8BJ2K1AUU",
        "outputId": "c257d4ed-55d4-481f-d4ba-3d90c9b3a97a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['woman', 'complain', 'clean', 'house', 'man', 'trash'],\n",
              " ['boy', 'dat', 'coldtyga', 'dwn', 'bad', 'cuffin', 'dat', 'hoe', 'place'],\n",
              " ['dawg', 'fuck', 'bitch', 'start', 'cry', 'confuse', 'shit'],\n",
              " ['look', 'like', 'tranny'],\n",
              " ['shit', 'hear', 'true', 'faker', 'bitch', 'tell'],\n",
              " ['shit', 'blow', 'meclaim', 'faithful', 'somebody', 'fuck', 'hoe'],\n",
              " ['sit', 'hate', 'bitch', 'shit'],\n",
              " ['cause', 'tired', 'big', 'bitch', 'come', 'skinny', 'girl'],\n",
              " ['bitch'],\n",
              " ['hobby', 'include', 'fight', 'mariam', 'bitch'],\n",
              " ['keek', 'bitch', 'curve', 'lol', 'walk', 'conversation', 'like', 'smh'],\n",
              " ['murda', 'gang', 'bitch', 'gang', 'land'],\n",
              " ['hoe', 'smoke', 'loser', 'yea'],\n",
              " ['bad', 'bitch', 'thing', 'like'],\n",
              " ['bitch']]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save preprocessed dataset\n",
        "pck = open(\"./pickles/preprocessed_dataset.pck\", \"wb\")\n",
        "pickle.dump({\n",
        "    'preprocessed_dataset': preprocessed_dataset,\n",
        "    'dataset_labels': dataset_labels\n",
        "}, pck)"
      ],
      "metadata": {
        "id": "LwQFhz3d4H2E"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_dataset1 = []\n",
        "dataset_labels1 = []\n",
        "\n",
        "pck = open(\"./pickles/preprocessed_dataset.pck\", \"rb\")\n",
        "result = pickle.load(pck)\n",
        "preprocessed_dataset1 = result['preprocessed_dataset']\n",
        "dataset_labels1 = result['dataset_labels']"
      ],
      "metadata": {
        "id": "4j9vqWw96nqn"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data in train, test, validation"
      ],
      "metadata": {
        "id": "RlPT0zmj4yHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val_test, y_train, y_val_test = train_test_split(preprocessed_dataset, dataset_labels, test_size=0.30, random_state=42)\n",
        "\n",
        "X_test, X_validation, y_test, y_validation = train_test_split(X_val_test, y_val_test, test_size=0.50, random_state=42)"
      ],
      "metadata": {
        "id": "mgCTUFko4trf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(X_validation))"
      ],
      "metadata": {
        "id": "OY9P2FMn4uhp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}